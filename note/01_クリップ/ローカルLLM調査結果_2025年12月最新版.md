# ローカルLLM調査結果（2025年12月最新版）

## 📊 最新ローカルLLMモデル比較表（2025年12月時点）

| モデル名 | パラメーター数 | 特徴 | 推奨VRAM | 推奨RAM | 推奨CPU | 量子化対応 | 日本語対応 |
|---------|--------------|------|---------|---------|---------|-----------|-----------|
| **Phi-4** | 14B（140億） | Microsoft Research開発。軽量ながら高度な推論性能。2024年12月リリース。 | 8GB以上（Q4_K_M） | 16GB以上 | Intel Core i7 / AMD Ryzen 7 | Q4_K_M, Q8_0対応 | 可（中程度） |
| **Llama 3.3 70B** | 70B（700億） | Meta社開発。英語圏で高い性能。密なモデル。 | 24GB以上（FP16）<br>12GB以上（Q4_K_M） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（中程度） |
| **Llama 3.1 8B** | 8B（80億） | Meta社開発。軽量で高速。エントリー〜ミドル向け。 | 8GB以上（FP16）<br>4GB以上（Q4_K_M） | 16GB以上 | Intel Core i7 / AMD Ryzen 7 | Q4_K_M, Q8_0対応 | 可（中程度） |
| **Qwen2.5 14B** | 14B（140億） | Alibaba社開発。多様なタスクに対応。 | 16GB以上（FP16）<br>8GB以上（Q4_K_M） | 32GB以上 | Intel Core i7 / AMD Ryzen 7 | Q4_K_M, Q8_0対応 | 可（良好） |
| **Qwen2.5 32B** | 32B（320億） | Alibaba社開発。コード生成・数学推論に特化。 | 24GB以上（FP16）<br>12GB以上（Q4_K_M） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（良好） |
| **Qwen2.5 72B** | 72B（720億） | Alibaba社開発。ハイエンド性能。 | 48GB以上（FP16）<br>24GB以上（Q4_K_M） | 128GB以上 | Intel Xeon / AMD Threadripper | Q4_K_M, Q8_0対応 | 可（良好） |
| **Qwen3-235B-A22B** | 235B総/22B活性 | Alibaba社開発。MoE型。アクティブパラメータ22B。 | 24GB以上（量子化後） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（良好） |
| **DeepSeek R1-0528** | 671B総/37B活性 | DeepSeek社開発。MoE型。推論特化型。2025年1月公開。 | 24GB以上（量子化後） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（中程度） |
| **Gemma-3n-E4B** | 8B総/4B活性 | Google DeepMind開発。軽量マルチモーダル。MatFormer構造。 | 3GB以上 | 8GB以上 | Intel Core i5以上 | Q4_K_M対応 | 可（中程度） |
| **GPT-OSS-20B** | 21B総/3.6B活性 | OpenAI初のオープンウェイト。MoE型。Harmony形式対応。 | 16GB以上 | 32GB以上 | Intel Core i7 / AMD Ryzen 7 | Q4_K_M対応 | 可（良好） |

---

## 🏆 ローカルLLM vs クラウドLLM ベンチマーク比較表（2025年12月時点）

### 主要ベンチマークスコア比較

| モデル名 | MMLU<br>（知識理解） | HumanEval<br>（コーディング） | MT-Bench<br>（対話能力） | GSM8K<br>（数学推論） | AIME 2025<br>（数学） | SWE-Bench<br>（実装能力） | MMLU-Pro<br>（推論力） |
|---------|-------------------|---------------------------|----------------------|-------------------|-------------------|---------------------|-------------------|
| **クラウドLLM** |
| GPT-5.2 Thinking | 85% | - | - | - | **100%** | 80.0% | - |
| Claude Opus 4.5 | 86% | - | - | - | 93% | **80.9%** | - |
| Claude 3.7 Sonnet | 88.7% | 71.2% | - | - | 42/60 | 67.3% | - |
| GPT-4 Turbo | 86.5% | 68.9% | - | - | 40/60 | 65.0% | - |
| GPT-4o | 89.0% | 75.7% | - | - | 45/60 | 74.0% | - |
| Gemini 2.5 Pro | 87.2% | 70.4% | - | - | 39/60 | 70.4% | - |
| Grok 3 | 87.5% | 70.6% | - | - | 52/60 | 70.6% | - |
| **ローカルLLM** |
| Qwen2.5 72B | **88%** | **85%** | 8.8 | - | - | - | - |
| Llama 3.3 70B | 86% | 83% | **9.0** | - | - | - | - |
| DeepSeek 67B | 85% | **86%** | 8.2 | - | - | - | - |
| Qwen3-235B-A22B-Thinking | - | - | - | - | 92.3% | - | **93.8%** |
| DeepSeek R1-0528 | - | - | - | - | **87.5%** | - | 93.4% |
| Phi-4 | 85% | **87%** | - | **81%** | - | - | - |
| GPT-OSS-20B | 72.3% | 67.5% | - | 81.7% | - | - | - |
| Qwen2.5 14B | 74% | 60% | 7.7 | - | - | - | - |
| Llama 3.3 8B | 70% | 54% | 7.5 | - | - | - | - |

**ベンチマーク説明:**
- **MMLU**: 多分野の知識理解能力（0-100%）
- **HumanEval**: Pythonコーディング能力（0-100%）
- **MT-Bench**: 多ターン対話能力（0-10）
- **GSM8K**: 数学的推論能力（0-100%）
- **AIME 2025**: 数学オリンピックレベルの問題（0-100%）
- **SWE-Bench**: 実際のソフトウェアバグ修正能力（0-100%）
- **MMLU-Pro**: 高度な推論能力（0-100%）

**情報源**: 
- [glbgpt.com](https://www.glbgpt.com/hub/jp/gpt-5-2-vs-claude-opus-4-5/)
- [skywork.ai](https://skywork.ai/blog/llm/top-10-open-llms-2025-november-ranking-analysis/)
- [qiita.com](https://qiita.com/Nomura_NT/items/dbbd7634bca08fd54dc9)
- [no1s.biz](https://no1s.biz/blog/8358/)

---

## 🎮 NVIDIA RTX 5000シリーズ スペック比較表（2025年最新）

| GPUモデル | VRAM | CUDAコア数 | メモリ帯域幅 | TDP | 参考価格 | ローカルLLM推奨モデル |
|---------|------|-----------|------------|-----|---------|-------------------|
| **RTX 5070** | 12GB GDDR7 | - | - | - | 約9万円<br>（$549） | 7B-8Bモデル（Q4_K_M）<br>14Bモデル（Q4_K_M） |
| **RTX 5070 Ti** | **16GB GDDR7** | 8,960 | 896 GB/s | 300W | 約12万円<br>（$749） | 14B-32Bモデル（Q4_K_M）<br>推論速度向上 |
| **RTX 5080** | **16GB GDDR7** | 10,752 | 960 GB/s | 360W | 約16万円<br>（$999） | 14B-32Bモデル（Q4_K_M）<br>高品質推論 |
| **RTX 5090** | **32GB GDDR7** | 21,760 | **1.8TB/s** | 575W | 約32万円<br>（$1999） | 30B-70Bモデル（FP16/Q8_0）<br>最大3,000トークン/秒 |

### RTX 5000シリーズのローカルLLM性能

| GPU | 推論速度（7Bモデル） | 推論速度（13Bモデル） | 画像生成速度 | RTX 4090比 |
|-----|------------------|-------------------|------------|-----------|
| **RTX 5090** | 最大3,000トークン/秒 | 約1,500トークン/秒 | 35画像/分 | **+50%** |
| **RTX 5080** | 約2,000トークン/秒 | 約1,000トークン/秒 | - | +30% |
| **RTX 5070 Ti** | 約1,500トークン/秒 | 約800トークン/秒 | - | +15% |

**情報源**: 
- [itmedia.co.jp](https://www.itmedia.co.jp/news/articles/2501/07/news119.html)
- [blogs.novita.ai](https://blogs.novita.ai/ja/is-the-rtx-5090-the-right-choice-for-ai-developers/)
- [gamemakers.jp](https://gamemakers.jp/article/2025_01_07_89735/)

---

## 💻 RTX 5000シリーズ別 推奨ローカルLLM構成

### RTX 5070（12GB VRAM）推奨構成
- **推奨モデル**: 
  - Llama 3.1 8B（Q4_K_M）
  - Qwen2.5 14B（Q4_K_M）
  - Phi-4（Q4_K_M）
- **RAM**: 32GB以上
- **CPU**: Intel Core i7 / AMD Ryzen 7
- **用途**: エントリー〜ミドル向け、軽量推論

### RTX 5070 Ti / RTX 5080（16GB VRAM）推奨構成
- **推奨モデル**: 
  - Qwen2.5 14B（Q8_0）
  - Qwen2.5 32B（Q4_K_M）
  - Llama 3.3 70B（Q4_K_M）
  - DeepSeek R1（Q4_K_M）
- **RAM**: 64GB以上
- **CPU**: Intel Core i9 / AMD Ryzen 9
- **用途**: ミドル〜ハイエンド、実用的な推論速度

### RTX 5090（32GB VRAM）推奨構成
- **推奨モデル**: 
  - Qwen2.5 72B（Q8_0）
  - Llama 3.3 70B（FP16）
  - Qwen3-235B-A22B（Q8_0）
  - DeepSeek R1（Q8_0）
- **RAM**: 128GB以上
- **CPU**: Intel Core i9 / AMD Ryzen 9以上
- **用途**: ハイエンド、最高品質推論、トレーニング

---

## 📈 推奨PCスペック（モデルサイズ別・RTX 5000対応）

### ミドルエンド（8B〜14Bパラメータ）

| コンポーネント | 推奨スペック | RTX 5000シリーズ対応 |
|--------------|------------|------------------|
| **GPU** | RTX 5070（12GB VRAM）<br>または RTX 5070 Ti（16GB VRAM） | ✅ 最適 |
| **VRAM** | 12GB以上（推奨16GB） | RTX 5070 Ti推奨 |
| **RAM** | 32GB以上 | DDR5推奨 |
| **CPU** | Intel Core i7 / AMD Ryzen 7以上 | マルチコア推奨 |
| **ストレージ** | 1TB NVMe SSD | PCIe 4.0以上 |

### ハイエンド（30B〜70Bパラメータ）

| コンポーネント | 推奨スペック | RTX 5000シリーズ対応 |
|--------------|------------|------------------|
| **GPU** | RTX 5090（32GB VRAM）<br>または RTX 5080（16GB VRAM + 量子化） | ✅ RTX 5090最適 |
| **VRAM** | 24GB以上（推奨32GB） | RTX 5090必須 |
| **RAM** | 64GB以上（推奨128GB） | DDR5推奨 |
| **CPU** | Intel Core i9 / AMD Ryzen 9以上 | 高クロック推奨 |
| **ストレージ** | 2TB以上 NVMe SSD | PCIe 5.0推奨 |

### 超ハイエンド（100B以上パラメータ）

| コンポーネント | 推奨スペック | RTX 5000シリーズ対応 |
|--------------|------------|------------------|
| **GPU** | RTX 5090 × 2（64GB VRAM合計）<br>または RTX PRO 5000 Blackwell（72GB VRAM） | ✅ 複数GPU推奨 |
| **VRAM** | 48GB以上（複数GPU合計） | モデル分割必須 |
| **RAM** | 128GB以上（推奨256GB以上） | ECCメモリ推奨 |
| **CPU** | Intel Xeon / AMD Threadripper | サーバー向けCPU |
| **ストレージ** | 4TB以上 NVMe SSD（RAID構成） | 高速I/O必須 |

---

## 🔬 量子化形式とVRAM必要量の目安（RTX 5000対応）

| 量子化形式 | 説明 | VRAM必要量（7Bモデル） | VRAM必要量（70Bモデル） | RTX 5000対応 |
|----------|------|---------------------|---------------------|------------|
| **FP16** | 16ビット浮動小数点（最高精度） | 約14GB | 約140GB | RTX 5090のみ |
| **INT8** | 8ビット整数 | 約7GB | 約70GB | RTX 5080以上 |
| **Q8_0** | 8ビット最適化形式 | 約7GB | 約70GB | RTX 5080以上 |
| **INT4** | 4ビット整数 | 約3.5GB | 約35GB | RTX 5070以上 |
| **Q4_K_M** | 4ビット最適化形式（推奨） | 約4GB | 約40GB | RTX 5070以上 |

**注意**: 推論時にはKVキャッシュなどの追加メモリが必要。実際の必要量は上記より多めに見積もることを推奨。

---

## 🌐 WindowsでローカルLLMサーバーを構築し、macOSからWi-Fi経由で接続する方法

### 1. Windows側のセットアップ（Ollama使用）

#### 1-1. Ollamaのインストール
1. [Ollama公式サイト](https://ollama.com/)からWindows版をダウンロード
2. インストーラーを実行してインストール
3. Ollamaを起動

#### 1-2. リモートアクセス設定

**環境変数の設定:**
1. 「システム環境変数の編集」を開く
2. 「ユーザー変数」または「システム変数」で「新規」をクリック
3. 以下を追加:
   - 変数名: `OLLAMA_HOST`
   - 変数値: `0.0.0.0`
4. 同様に以下を追加:
   - 変数名: `OLLAMA_ORIGINS`
   - 変数値: `*`
5. Windowsを再起動して変更を適用

**ファイアウォール設定:**
1. 「セキュリティが強化されたWindows Defenderファイアウォール」を開く
2. 「受信の規則」→「新しい規則」
3. 「ポート」を選択
4. 「特定のローカルポート」に`11434`を入力
5. 「接続を許可する」を選択
6. 適用するプロファイル（プライベート、パブリック）を選択
7. 規則名を付けて完了

#### 1-3. WindowsマシンのIPアドレス確認
```cmd
ipconfig
```
Wi-Fi接続の「IPv4アドレス」を確認（例: 192.168.1.100）

#### 1-4. モデルのダウンロード
```cmd
ollama pull llama3.1:8b
# または
ollama pull qwen2.5:14b
```

### 2. macOS側からの接続

#### 2-1. 接続確認
```bash
curl http://<WindowsマシンのIPアドレス>:11434
```
例: `curl http://192.168.1.100:11434`

#### 2-2. モデル使用
```bash
curl http://192.168.1.100:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "こんにちは",
  "stream": false
}'
```

#### 2-3. macOS側でOllamaクライアントを使用する場合
環境変数を設定:
```bash
export OLLAMA_HOST=http://192.168.1.100:11434
```

その後、通常通りOllamaコマンドが使用可能:
```bash
ollama run llama3.1:8b
```

### 3. セキュリティ注意事項

⚠️ **重要**: Ollamaには認証機能がないため、以下の点に注意:

1. **信頼できるネットワーク内でのみ使用**
   - 自宅のWi-Fiネットワークなど、信頼できる環境でのみ使用
   - 公共Wi-Fiでは使用しない

2. **VPNの利用を検討**
   - より安全に接続したい場合は、VPN経由での接続を検討

3. **リバースプロキシの利用**
   - Nginxなどのリバースプロキシを使用して認証を追加可能

4. **ファイアウォール設定の見直し**
   - 必要に応じて特定のIPアドレスからのみアクセスを許可

---

## 💰 推奨構成例（RTX 5000シリーズ対応）

### ミドルエンド構成（予算: 20〜30万円）
- **GPU**: NVIDIA RTX 5070 Ti（16GB VRAM）
- **CPU**: AMD Ryzen 7 7700X
- **RAM**: 32GB DDR5
- **ストレージ**: 1TB NVMe SSD
- **推奨モデル**: Llama 3.1 8B, Qwen2.5 14B, Phi-4（Q4_K_M量子化）

### ハイエンド構成（予算: 50〜70万円）
- **GPU**: NVIDIA RTX 5090（32GB VRAM）
- **CPU**: AMD Ryzen 9 7950X
- **RAM**: 64GB DDR5
- **ストレージ**: 2TB NVMe SSD
- **推奨モデル**: Llama 3.3 70B, Qwen2.5 32B, DeepSeek R1（Q8_0量子化）

### 超ハイエンド構成（予算: 120万円以上）
- **GPU**: NVIDIA RTX 5090 × 2（64GB VRAM合計）
- **CPU**: AMD Threadripper 7960X
- **RAM**: 128GB DDR5
- **ストレージ**: 4TB NVMe SSD（RAID 0）
- **推奨モデル**: Qwen2.5 72B, Qwen3-235B-A22B（FP16またはQ8_0）

---

## 📊 まとめ

### ローカルLLMの現状（2025年12月）
- **ミドルエンド**: 8B〜14BモデルがRTX 5070 Ti（16GB VRAM）で快適に動作
- **ハイエンド**: 30B〜70BモデルはRTX 5090（32GB VRAM）推奨
- **量子化**: Q4_K_M形式を使用することで、VRAM必要量を大幅に削減可能
- **性能**: クラウドLLM（GPT-5.2、Claude Opus 4.5）には劣るが、Qwen2.5 72BやDeepSeek R1は高い推論能力を発揮

### クラウドLLMとの比較
- **コーディング能力**: ローカルLLM（Qwen2.5 72B: 85%）はGPT-4o（75.7%）を上回る
- **数学推論**: DeepSeek R1-0528（AIME 87.5%）は高い性能を示すが、GPT-5.2（100%）には及ばない
- **知識理解**: Qwen2.5 72B（MMLU 88%）はClaude 3.7 Sonnet（88.7%）に近い性能

### RTX 5000シリーズの優位性
- **RTX 5090**: 32GB VRAMで70BモデルをFP16で動作可能、推論速度はRTX 4090比+50%
- **RTX 5080/5070 Ti**: 16GB VRAMで32Bモデルを量子化で動作可能、コストパフォーマンス良好
- **リモートアクセス**: Ollamaを使用すれば、WindowsサーバーとmacOSクライアント間の接続が可能
- **セキュリティ**: 信頼できるネットワーク内でのみ使用し、必要に応じてVPNやリバースプロキシを検討

---

## 📚 参考リンク

- [Ollama公式サイト](https://ollama.com/)
- [Llama 3.3公式](https://llama.meta.com/)
- [Qwen公式](https://qwenlm.com/)
- [DeepSeek公式](https://www.deepseek.com/)
- [NVIDIA RTX 5000シリーズ](https://www.nvidia.com/ja-jp/geforce/graphics-cards/50-series/)

---

**調査日**: 2025年12月  
**情報源**: 
- 各種技術ブログ、公式ドキュメント、ベンチマーク結果
- glbgpt.com, skywork.ai, qiita.com, no1s.biz
- itmedia.co.jp, blogs.novita.ai, gamemakers.jp
