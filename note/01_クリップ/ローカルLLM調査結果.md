# ローカルLLM調査結果（2025年12月最新版）

## 最新ローカルLLMモデル比較表（ミドルエンド〜ハイエンド）

| モデル名 | パラメーター数 | 特徴 | 推奨VRAM | 推奨RAM | 推奨CPU | 量子化対応 | 日本語対応 |
|---------|--------------|------|---------|---------|---------|-----------|-----------|
| **Llama 3.3 70B** | 70B（700億） | Meta社開発。英語圏で高い性能。密なモデル。 | 24GB以上（FP16）<br>12GB以上（Q4_K_M） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（中程度） |
| **Llama 3.1 8B** | 8B（80億） | Meta社開発。軽量で高速。エントリー〜ミドル向け。 | 8GB以上（FP16）<br>4GB以上（Q4_K_M） | 16GB以上 | Intel Core i7 / AMD Ryzen 7 | Q4_K_M, Q8_0対応 | 可（中程度） |
| **Qwen2.5 14B** | 14B（140億） | Alibaba社開発。多様なタスクに対応。 | 16GB以上（FP16）<br>8GB以上（Q4_K_M） | 32GB以上 | Intel Core i7 / AMD Ryzen 7 | Q4_K_M, Q8_0対応 | 可（良好） |
| **Qwen2.5 32B** | 32B（320億） | Alibaba社開発。コード生成・数学推論に特化。 | 24GB以上（FP16）<br>12GB以上（Q4_K_M） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（良好） |
| **Qwen2.5 72B** | 72B（720億） | Alibaba社開発。ハイエンド性能。 | 48GB以上（FP16）<br>24GB以上（Q4_K_M） | 128GB以上 | Intel Xeon / AMD Threadripper | Q4_K_M, Q8_0対応 | 可（良好） |
| **Qwen3-235B-A22B** | 235B総/22B活性 | Alibaba社開発。MoE型。アクティブパラメータ22B。 | 24GB以上（量子化後） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（良好） |
| **DeepSeek R1** | 671B総/37B活性 | DeepSeek社開発。MoE型。中国語・教育・法律分野で高評価。 | 24GB以上（量子化後） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（中程度） |
| **Mistral Large 2** | 123B（1230億） | Mistral AI開発。多言語対応。 | 48GB以上（FP16）<br>24GB以上（Q4_K_M） | 128GB以上 | Intel Xeon / AMD Threadripper | Q4_K_M, Q8_0対応 | 可（良好） |
| **Mistral 3** | 675B総/41B活性 | Mistral AI開発。MoE型。最新モデル。 | 24GB以上（量子化後） | 64GB以上 | Intel Core i9 / AMD Ryzen 9 | Q4_K_M, Q8_0対応 | 可（良好） |

## 推奨PCスペック（モデルサイズ別）

### ミドルエンド（8B〜14Bパラメータ）

| コンポーネント | 推奨スペック | 備考 |
|--------------|------------|------|
| **GPU** | NVIDIA RTX 4070（12GB VRAM）以上<br>または RTX 3060（12GB VRAM） | 量子化使用時は8GB VRAMでも可 |
| **VRAM** | 12GB以上（推奨16GB） | Q4_K_M量子化で8GBでも動作可能 |
| **RAM** | 32GB以上 | システム全体の安定動作に必要 |
| **CPU** | Intel Core i7 / AMD Ryzen 7以上 | マルチコア推奨 |
| **ストレージ** | 1TB NVMe SSD | モデルダウンロード・キャッシュ用 |

### ハイエンド（30B〜70Bパラメータ）

| コンポーネント | 推奨スペック | 備考 |
|--------------|------------|------|
| **GPU** | NVIDIA RTX 4090（24GB VRAM）<br>または複数GPU構成 | 70Bモデルは複数GPU推奨 |
| **VRAM** | 24GB以上（推奨48GB） | Q4_K_M量子化で12GBでも動作可能 |
| **RAM** | 64GB以上（推奨128GB） | 大規模モデルには必須 |
| **CPU** | Intel Core i9 / AMD Ryzen 9以上<br>または Intel Xeon / AMD Threadripper | 高負荷処理に対応 |
| **ストレージ** | 2TB以上 NVMe SSD | RAID構成推奨 |

### 超ハイエンド（100B以上パラメータ）

| コンポーネント | 推奨スペック | 備考 |
|--------------|------------|------|
| **GPU** | 複数GPU構成（RTX 4090×2以上）<br>または A100/H100 | エンタープライズ向け |
| **VRAM** | 48GB以上（複数GPU合計） | モデル分割が必要 |
| **RAM** | 128GB以上（推奨256GB以上） | 大規模推論に必須 |
| **CPU** | Intel Xeon / AMD Threadripper | サーバー向けCPU推奨 |
| **ストレージ** | 4TB以上 NVMe SSD（RAID構成） | 高速I/O必須 |

## 量子化形式とVRAM必要量の目安

| 量子化形式 | 説明 | VRAM必要量（7Bモデル例） | VRAM必要量（70Bモデル例） |
|----------|------|----------------------|----------------------|
| **FP16** | 16ビット浮動小数点（最高精度） | 約14GB | 約140GB |
| **INT8** | 8ビット整数 | 約7GB | 約70GB |
| **Q8_0** | 8ビット最適化形式 | 約7GB | 約70GB |
| **INT4** | 4ビット整数 | 約3.5GB | 約35GB |
| **Q4_K_M** | 4ビット最適化形式（推奨） | 約4GB | 約40GB |

**注意**: 推論時にはKVキャッシュなどの追加メモリが必要。実際の必要量は上記より多めに見積もることを推奨。

## WindowsでローカルLLMサーバーを構築し、macOSからWi-Fi経由で接続する方法

### 1. Windows側のセットアップ（Ollama使用）

#### 1-1. Ollamaのインストール
1. [Ollama公式サイト](https://ollama.com/)からWindows版をダウンロード
2. インストーラーを実行してインストール
3. Ollamaを起動

#### 1-2. リモートアクセス設定

**環境変数の設定:**
1. 「システム環境変数の編集」を開く
2. 「ユーザー変数」または「システム変数」で「新規」をクリック
3. 以下を追加:
   - 変数名: `OLLAMA_HOST`
   - 変数値: `0.0.0.0`
4. 同様に以下を追加:
   - 変数名: `OLLAMA_ORIGINS`
   - 変数値: `*`
5. Windowsを再起動して変更を適用

**ファイアウォール設定:**
1. 「セキュリティが強化されたWindows Defenderファイアウォール」を開く
2. 「受信の規則」→「新しい規則」
3. 「ポート」を選択
4. 「特定のローカルポート」に`11434`を入力
5. 「接続を許可する」を選択
6. 適用するプロファイル（プライベート、パブリック）を選択
7. 規則名を付けて完了

#### 1-3. WindowsマシンのIPアドレス確認
```cmd
ipconfig
```
Wi-Fi接続の「IPv4アドレス」を確認（例: 192.168.1.100）

#### 1-4. モデルのダウンロード
```cmd
ollama pull llama3.1:8b
# または
ollama pull qwen2.5:14b
```

### 2. macOS側からの接続

#### 2-1. 接続確認
```bash
curl http://<WindowsマシンのIPアドレス>:11434
```
例: `curl http://192.168.1.100:11434`

#### 2-2. モデル使用
```bash
curl http://192.168.1.100:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "こんにちは",
  "stream": false
}'
```

#### 2-3. macOS側でOllamaクライアントを使用する場合
環境変数を設定:
```bash
export OLLAMA_HOST=http://192.168.1.100:11434
```

その後、通常通りOllamaコマンドが使用可能:
```bash
ollama run llama3.1:8b
```

### 3. セキュリティ注意事項

⚠️ **重要**: Ollamaには認証機能がないため、以下の点に注意:

1. **信頼できるネットワーク内でのみ使用**
   - 自宅のWi-Fiネットワークなど、信頼できる環境でのみ使用
   - 公共Wi-Fiでは使用しない

2. **VPNの利用を検討**
   - より安全に接続したい場合は、VPN経由での接続を検討

3. **リバースプロキシの利用**
   - Nginxなどのリバースプロキシを使用して認証を追加可能

4. **ファイアウォール設定の見直し**
   - 必要に応じて特定のIPアドレスからのみアクセスを許可

## 推奨構成例

### ミドルエンド構成（予算: 15〜25万円）
- **GPU**: NVIDIA RTX 4070（12GB VRAM）
- **CPU**: AMD Ryzen 7 7700X
- **RAM**: 32GB DDR5
- **ストレージ**: 1TB NVMe SSD
- **推奨モデル**: Llama 3.1 8B, Qwen2.5 14B（Q4_K_M量子化）

### ハイエンド構成（予算: 40〜60万円）
- **GPU**: NVIDIA RTX 4090（24GB VRAM）
- **CPU**: AMD Ryzen 9 7950X
- **RAM**: 64GB DDR5
- **ストレージ**: 2TB NVMe SSD
- **推奨モデル**: Llama 3.3 70B, Qwen2.5 32B（Q4_K_M量子化）

### 超ハイエンド構成（予算: 100万円以上）
- **GPU**: NVIDIA RTX 4090 × 2（48GB VRAM合計）
- **CPU**: AMD Threadripper 7960X
- **RAM**: 128GB DDR5
- **ストレージ**: 4TB NVMe SSD（RAID 0）
- **推奨モデル**: Qwen2.5 72B, DeepSeek R1（FP16またはQ8_0）

## まとめ

- **ミドルエンド**: 8B〜14BモデルがRTX 4070（12GB VRAM）で快適に動作
- **ハイエンド**: 30B〜70BモデルはRTX 4090（24GB VRAM）推奨
- **量子化**: Q4_K_M形式を使用することで、VRAM必要量を大幅に削減可能
- **リモートアクセス**: Ollamaを使用すれば、WindowsサーバーとmacOSクライアント間の接続が可能
- **セキュリティ**: 信頼できるネットワーク内でのみ使用し、必要に応じてVPNやリバースプロキシを検討

## 参考リンク

- [Ollama公式サイト](https://ollama.com/)
- [Llama 3.3公式](https://llama.meta.com/)
- [Qwen公式](https://qwenlm.com/)
- [DeepSeek公式](https://www.deepseek.com/)

---

**調査日**: 2025年1月
**情報源**: 各種技術ブログ、公式ドキュメント、ベンチマーク結果
